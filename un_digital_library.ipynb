{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the United Nation's Digital Library\n",
    "\n",
    "**Scraping script done on behalf of Oladoyin Okunoren @ Boston College**\n",
    "\n",
    "By David J. Thomas\n",
    "\n",
    "---\n",
    "\n",
    "This notebook contains a series of scripts to scrape every item from the United Nation's Digital Librarys database about Ebola from 2014-2016. It is a part of the dissertation of research of Oladoyin Okunoren, at [Boston College](https://bc.edu)\n",
    "\n",
    "---\n",
    "\n",
    "## Installation\n",
    "\n",
    "``` bash\n",
    "pip install -r requirements.txt\n",
    "jupyter lab\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function defined! PROCEED\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class BasePageScraper:\n",
    "    \"\"\"Gets HTML for a single page, parses it with BeautifulSoup, and stores results in self.data.\n",
    "    Base class for child classes which target specific pages\"\"\"\n",
    "    \n",
    "    def __init__(self, url, total_tries=5, scrape_delay=0.5):\n",
    "        \"\"\"Gets url and pre-parses the html\"\"\"\n",
    "        self._url = url\n",
    "        self._total_tries = total_tries\n",
    "        self._scrape_delay = scrape_delay\n",
    "        # store souped data in object for extraction\n",
    "        self.data = self.soup_page(tries_left=self._total_tries)\n",
    "\n",
    "    def soup_page(self, tries_left=5):\n",
    "        \"\"\"Receives URL, requests raw html, then returns converted BeautifulSoup object.\"\"\"\n",
    "        # declare variable for raw html\n",
    "        page_html = None\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0\",\n",
    "            \"Accept-Encoding\": \"*\",\n",
    "            \"Connection\": \"keep-alive\"\n",
    "        }\n",
    "        # ensure tries_left is set and valid, if not set to 5, check if url is valid\n",
    "        if not tries_left or type(tries_left) != int or tries_left < 0:\n",
    "            tries_left = 5\n",
    "        if type(self._url) != str:\n",
    "            raise Exception('URL must be a valid string')\n",
    "        # enforce a time delay between each scrape for good internet citizenship\n",
    "        time.sleep(self._scrape_delay)\n",
    "        print('Getting', self._url)\n",
    "        # attempt to get page data, decrement tries_left if successful\n",
    "        try:\n",
    "            page_html = requests.get(self._url, headers=headers).text\n",
    "            tries_left -= 1\n",
    "        # if an error occured, retry by returning recursively\n",
    "        except:\n",
    "            print('Error getting', self._url)\n",
    "            if tries_left > 0:\n",
    "                print('Retrying...')\n",
    "                print(tries_left)\n",
    "                return self.soup_page(self._url, tries_left=tries_left-1)\n",
    "            if tries_left <= 0:\n",
    "                print('Retry limit reached, ABORTING parse of', self._url)\n",
    "                return None\n",
    "        print('Success, souping...')\n",
    "        # if all went well, return new BeautifulSoup populated with the page html and parser set to html.parser\n",
    "        return BeautifulSoup(page_html, 'html.parser')\n",
    "\n",
    "print('Function defined! PROCEED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering stories, this may take awhile...\n",
      "Getting https://digitallibrary.un.org/search?p=ebola&c=Resource+Type&c=UN+Bodies&rg=50&jrec=51&fct__3=2014&fct__3=2015&fct__3=2016&ln=en\n",
      "Success, souping...\n",
      "Getting https://digitallibrary.un.org/search?p=ebola&c=Resource+Type&c=UN+Bodies&rg=50&jrec=101&fct__3=2014&fct__3=2015&fct__3=2016&ln=en\n",
      "Success, souping...\n",
      "Getting https://digitallibrary.un.org/search?p=ebola&c=Resource+Type&c=UN+Bodies&rg=50&jrec=151&fct__3=2014&fct__3=2015&fct__3=2016&ln=en\n",
      "Success, souping...\n",
      "Getting https://digitallibrary.un.org/search?p=ebola&c=Resource+Type&c=UN+Bodies&rg=50&jrec=201&fct__3=2014&fct__3=2015&fct__3=2016&ln=en\n",
      "Success, souping...\n",
      "Getting https://digitallibrary.un.org/search?p=ebola&c=Resource+Type&c=UN+Bodies&rg=50&jrec=251&fct__3=2014&fct__3=2015&fct__3=2016&ln=en\n",
      "Success, souping...\n",
      "227 links to stories gathered\n",
      "['https://digitallibrary.un.org/record/795883?v=pdf#files', 'https://digitallibrary.un.org/record/783809?v=pdf#files', 'https://digitallibrary.un.org/record/807537?v=pdf#files', 'https://digitallibrary.un.org/record/782800?v=pdf#files', 'https://digitallibrary.un.org/record/783100?v=pdf#files']\n"
     ]
    }
   ],
   "source": [
    "class UNDLBrowseScraper(BasePageScraper):\n",
    "    \"\"\"Represents a browse page of the Awoko News Papers. Gathers links of stories returned from a search\"\"\"\n",
    "\n",
    "    def __init__(self, url, total_tries=5, scrape_delay=0.2):\n",
    "        super().__init__(url=url, total_tries=total_tries, scrape_delay=scrape_delay)\n",
    "\n",
    "    @property\n",
    "    def next_link(self):\n",
    "        \"\"\"Returns link to the next browse page if exists, or None if no further page\"\"\"\n",
    "        link = None\n",
    "        link_containers = self.data.find('span', class_='rec-navigation').find_all('a', class_='img')\n",
    "        for link_container in link_containers:\n",
    "            if link_container.img['alt'] == 'next':\n",
    "                link = link_container['href']\n",
    "        if link is None:\n",
    "            return None\n",
    "        return 'https://digitallibrary.un.org' + link\n",
    "\n",
    "    @property\n",
    "    def links(self):\n",
    "        \"\"\"Peruses the souped data and returns list of strings, each with a link\"\"\"\n",
    "        return_links = []\n",
    "        for article in self.data.find_all('div', class_='result-title'):\n",
    "            # append link of article to list\n",
    "            return_links.append('https://digitallibrary.un.org' + article.a['href'] + '?v=pdf#files')\n",
    "        return return_links\n",
    "    \n",
    "    def gather_links(self):\n",
    "        \"\"\"Recursive function to gather links to all stories on this page, and subsequent pages. If not on the last page,\n",
    "        return the links on the page plus those returned by a recursively call another AwokoBrowseScraper object on the next page.\n",
    "        If on the last page, just return the links and break the recursive loop\"\"\"\n",
    "        # if no next_link, just return the links on the page\n",
    "        if not self.next_link:\n",
    "            return self.links\n",
    "        else:\n",
    "            return self.links + UNDLBrowseScraper(self.next_link).gather_links()\n",
    "\n",
    "article_links = []\n",
    "print('Gathering stories, this may take awhile...')\n",
    "# gather stories for 2014, 2015, 2016\n",
    "article_links = UNDLBrowseScraper('https://digitallibrary.un.org/search?p=ebola&c=Resource+Type&c=UN+Bodies&rg=50&jrec=51&fct__3=2014&fct__3=2015&fct__3=2016&ln=en').gather_links()\n",
    "\n",
    "print(str(len(article_links)) + ' links to stories gathered')\n",
    "print(article_links[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class defined, PROCEED.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "class BaseSeleniumScraper:\n",
    "    \"\"\"Base class for all Selenium Scraper objects.\"\"\"\n",
    "    url = ''\n",
    "    scrape_delay = 5\n",
    "    options = Options()\n",
    "    webdriver_path = '/usr/local/bin/chromedriver'\n",
    "    service = None\n",
    "    driver = None\n",
    "    metadata = {}\n",
    "\n",
    "\n",
    "    def __init__(self, url,  *args, **kwargs):\n",
    "        # store the target url\n",
    "        self.url = url\n",
    "        # store scrape delay, if provided and a valid number\n",
    "        if 'scrape_delay' in kwargs:\n",
    "            if type(kwargs['scrape_delay']) == 'int' or type(kwargs['scrape_delay']) == 'float' or type(kwargs['scrape_delay']) == 'complex':\n",
    "                self.scrape_delay = kwargs['scrape_delay']\n",
    "            else:\n",
    "                raise Exception('Argument \\'scrape_delay must be an integer\\', float, or complex number')\n",
    "        # store webdriver_path, if a string and pointing to a file\n",
    "        if 'webdriver_path' in kwargs:\n",
    "            if type(kwargs['webdriver_path']) != 'string':\n",
    "                raise Exception('Argument \\'webdriver_path\\' must be a string')\n",
    "            if not os.path.isfile(os.path.abspath(kwargs['webdriver_path'])):\n",
    "                raise Exception('Argument \\'webdriver_path\\' must point to a valid webdriver')\n",
    "            self.webdriver_path = kwargs['webdriver_path']\n",
    "        # comment out this line to run Chrome normally\n",
    "        # self.options.add_argument('--headless')\n",
    "        self.service = ChromeService(executable_path=self.webdriver_path)\n",
    "        self.driver = webdriver.Chrome(service=self.service, options=self.options)\n",
    "        self.load()\n",
    "        # make sure to shutdown the driver even if error occurs\n",
    "        try:\n",
    "            self.post_load()\n",
    "        except Exception as e:\n",
    "            self.shutdown()\n",
    "            raise Exception(e)\n",
    "        self.shutdown()\n",
    "\n",
    "    def load(self, *args, **kwargs):\n",
    "        \"\"\"Enforces a per-page scraping delay, then performs the inital page load.\"\"\"\n",
    "        time.sleep(self.scrape_delay)\n",
    "        # fetch the page data\n",
    "        print('Getting page at ', self.url)\n",
    "        self.driver.get(self.url)\n",
    "\n",
    "    def post_load(self, *args, **kwargs):\n",
    "        \"\"\"Runs after the initial load of page data. Placeholder, SHOULD BE OVERWRITTEN by child classes to extract data\"\"\"\n",
    "        pass\n",
    "\n",
    "    def shutdown(self, *args, **kwargs):\n",
    "        self.driver.quit()\n",
    "\n",
    "print('Class defined, PROCEED.')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting page at  https://digitallibrary.un.org/record/795883?v=pdf#files\n",
      "Getting PDF at  https://digitallibrary.un.org/record/795883/files/A_HRC_28_L-31_Rev-1-EN.pdf\n",
      "GE.15-06370 (E) 260315 260315  Human Rights Council Twenty-eighth session Agenda item 10 Technical assistance and capacity-building Algeria (on behalf of the Group of African States), Bulgaria*, Cyprus*, Germany, Greece*, Hungary*, Italy*, Latvia, Luxembourg*, Netherlands, Slovakia*, United Kingdom of Great Britain and Northern Ireland: draft resolution 28/... Strengthening of technical cooperation and consultative services in Guinea The Human Rights Council, Guided by the Charter of the United Nations, the Universal Declaration of Human Rights and other applicable human rights instruments, Recalling General Assembly resolution 60/251 of 15 March 2006 and Human Rights Council resolutions 13/21 of 26 March 2010, 16/36 of 25 March 2011, 19/30 of 23 March 2012, 23/23 of 14 June 2013 and 25/35 of 28 March 2014, Reaffirming that all States have an obligation to promote and protect the human rights and fundamental freedoms set forth in the Charter of the United Nations, the Universal Declaration of Human Rights, the International Covenants on Human Rights and other relevant human rights instruments to which they are parties, Noting with satisfaction the efforts made by Guineans and the international community, in particular the African Union and the Economic Community of West African States, to establish democratic institutions and to strengthen the rule of law, Welcoming the progress made by the Guinean authorities in guaranteeing freedom of opinion and expression, Recalling the recommendations of the International Commission of Inquiry established by the Secretary-General and supported by the African Union and the Economic Community of West African States, * Non-member State of the Human Rights Council. United Nations A/HRC/28/L.31/Rev.1 General Assembly Distr.: Limited 25 March 2015 English Original: French A/HRC/28/L.31/Rev.1 2 GE.15-06370 Recalling that it is the primary responsibility of the Government of Guinea to protect its population, to conduct inquiries into allegations of human rights violations and to bring perpetrators to justice, 1. Recognizes the efforts made by the Government of Guinea to strengthen the rule of law and improve the human rights situation in the country; 2. Welcomes the existence of the new Ministry of Human Rights and Civil Liberties and its constructive work and the mainstreaming of human rights in the reform of the security sector; 3. Encourages the Guinean authorities to mainstream human rights in all public policies; 4. Calls upon the Guinean authorities to pursue their efforts to guarantee freedom of peaceful assembly and association; 5. Also calls upon the Guinean authorities to make the justice, truth and reconciliation process operational; 6. Firmly reiterates its commitment to accession to power by democratic means and condemns all incitement to ethnic and/or racial hatred; 7. Calls upon the Government of Guinea to ensure that the elections to be held in 2015 take place within the specified time frame, in conditions of peace, transparency and security, and with full respect for human rights and democratic principles; 8. Urges all political stakeholders: (a) To continue to take an active part in good faith in the political dialogue, particularly on issues regarding the organization of free, transparent, inclusive and peaceful elections; (b) To prevent any act of violence that harms the democratization process under way and to refrain from committing such acts; (c) To be actively involved in the national reconciliation process; 9. Encourages the Government of Guinea to make the national commission established in 2013 for the study and prevention of the problem of violence operational; 10. Welcomes the efforts by the Government of Guinea to undertake reforms in the security and defence sectors that incorporate respect for human rights and guarantee the enjoyment of civil and political rights and calls upon the Government of Guinea to continue human rights training for the security forces; 11. Welcomes the progress made with regard to the reform of the justice sector, including the establishment of the Supreme Council of Justice and the enhancement of judges’ conditions of service; 12. Encourages the Government of Guinea to adopt and carry out reforms aimed at strengthening the administration of justice in order to combat impunity and consolidate respect for human rights; 13. Calls upon the Government of Guinea to ensure that the National Human Rights Institution that has been established is compliant with the Paris Principles; 14. Encourages the Government of Guinea to pursue its efforts to fight impunity, in particular the judicial proceedings initiated in connection with the violence allegedly committed by the security forces, notably in 2007 and 2013; A/HRC/28/L.31/Rev.1 GE.15-06370 3 15. Urges the Government of Guinea to adopt the following supplementary measures: (a) Support the work of the panel of judges appointed to investigate the events of 28 September 2009, and expedite judicial proceedings against those responsible for the violence, including the acts of sexual violence committed against women and girls; (b) Guarantee the necessary means and security conditions for the panel of judges so that its members may effectively fulfil their mandate; (c) Ensure the safety and protection of the witnesses and victims and provide them with appropriate redress and assistance, including in the form of medical and psychological support; (d) Compensate the families of victims who lost their lives as a result of the events of 28 September 2009 and provide redress for the physical and psychological suffering inflicted upon those who were wounded; 16. Takes note of the report of the United Nations High Commissioner for Human Rights on the situation of human rights in Guinea in 2014;1 17. Firmly reiterates its appeal to the international community: (a) To provide the Government of Guinea with appropriate assistance to promote respect for human rights, the fight against impunity and the reform of the security and justice sectors, as well as the initiatives that are under way to promote truth, justice and national reconciliation; (b) To support the efforts of the Government of Guinea to combat the Ebola virus and strengthen the resilience of its health-care system; (c) To support the Office of the United Nations High Commissioner for Human Rights in Guinea; (d) To support the Ministry of Human Rights and Civil Liberties in the implementation of its plan of action; 18. Invites the High Commissioner to report to the Human Rights Council at its thirty-first session on the situation of human rights and the work of the Office of the High Commissioner in Guinea; 19. Decides to remain seized of this matter. 1 A/HRC/28/50.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import fitz\n",
    "\n",
    "class PDFScraper:\n",
    "    \"\"\"Separate scraper to handle fetching/converting the PDF into text. Will be used by the UNDLArticleScraper below\"\"\"\n",
    "    url = ''\n",
    "    scrape_delay = 5\n",
    "    max_tries = 5\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0\",\n",
    "            \"Accept-Encoding\": \"*\",\n",
    "            \"Connection\": \"keep-alive\"\n",
    "        }\n",
    "    data = None\n",
    "\n",
    "    def __init__(self, url, *args, **kwargs):\n",
    "        if type(url) != str:\n",
    "            raise Exception('URL must be a valid string')\n",
    "        self.url = url\n",
    "        # store scrape delay, if provided and a valid number\n",
    "        if 'scrape_delay' in kwargs:\n",
    "            if type(kwargs['scrape_delay']) != 'int' and type(kwargs['scrape_delay']) != 'float' and type(kwargs['scrape_delay']) == 'complex':\n",
    "                raise Exception('Argument \\'scrape_delay must be an integer\\', float, or complex number')\n",
    "            self.scrape_delay = kwargs['scrape_delay']\n",
    "        # store scrape delay, if provided and a valid number\n",
    "        if 'max_tries' in kwargs:\n",
    "            if type(kwargs['max_tries']) != 'int' and type(kwargs['max_tries']) != 'float' and type(kwargs['max_tries']) == 'complex':\n",
    "                raise Exception('Argument \\'scrape_delay must be an integer\\', float, or complex number')\n",
    "            self.max_tries = kwargs['max_tries']\n",
    "        self.data = self.pdf\n",
    "\n",
    "    def get_pdf_data(self, tries_left=5):\n",
    "        \"\"\"Fetch data and returns raw content... if fail to fetch, returns self recursively, with tries_left decremented\"\"\"\n",
    "        # enforce scrape delay\n",
    "        pdf_content = None\n",
    "        time.sleep(self.scrape_delay)\n",
    "        if not tries_left or type(tries_left) != int or tries_left < 0:\n",
    "            tries_left = 5\n",
    "        print('Getting PDF at ', self.url)\n",
    "        # attempt to get page data, decrement tries_left if successful\n",
    "        try:\n",
    "            pdf_content = requests.get(self.url, headers=self.headers).content\n",
    "            tries_left -= 1\n",
    "         # if an error occured, retry by returning recursively\n",
    "        except:\n",
    "            print('Error getting', self.url)\n",
    "            if tries_left > 0:\n",
    "                print('Retrying...')\n",
    "                print(tries_left)\n",
    "                return self.get_pdf_data(self.url, tries_left=tries_left-1)\n",
    "            if tries_left <= 0:\n",
    "                print('Retry limit reached, ABORTING parse of', self.url)\n",
    "                return None\n",
    "        return pdf_content\n",
    "    \n",
    "    @property\n",
    "    def pdf(self):\n",
    "        \"\"\"Returns the content of the PDF as text\"\"\"\n",
    "        text = ''\n",
    "        pdf_document = fitz.open('pdf', self.get_pdf_data(self.max_tries))\n",
    "        for page_num in range(pdf_document.page_count):\n",
    "            page = pdf_document[page_num]\n",
    "            text += page.get_text()\n",
    "        # return text with redundant whitespace and all newlines/tabs replaced\n",
    "        text = text.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ')\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "\n",
    "class UNDLArticleSeleniumScraper(BaseSeleniumScraper):\n",
    "    \"\"\"Used to scrape the metadata & pdf link from a single article page. Uses the PDF scraper to get contents of the PDF\"\"\"\n",
    "\n",
    "    def post_load(self, *args, **kwargs):\n",
    "        wait = WebDriverWait(self.driver, self.scrape_delay)\n",
    "        metadata_container = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div.detailed-record-content'))).get_attribute('innerHTML')\n",
    "        time.sleep(self.scrape_delay)\n",
    "        files_container = wait.until(EC.presence_of_element_located((By.ID, 'record-files-list'))).get_attribute('innerHTML')\n",
    "        metadata_soup = BeautifulSoup(metadata_container, 'html.parser')\n",
    "        files_soup = BeautifulSoup(files_container, 'html.parser')\n",
    "        self.metadata = self.get_metadata(metadata_soup)\n",
    "        self.metadata['enPdfLink'] = self.get_en_pdf_link(files_soup)\n",
    "\n",
    "    def get_metadata(self, souped_data, *args, **kwargs):\n",
    "        \"\"\"Extract metadata from page after load and return as a dictionary\"\"\"\n",
    "        extracted_data = {\n",
    "            'title': '',\n",
    "            'authors': '',\n",
    "            'description': '',\n",
    "            'agenda': '',\n",
    "            'resolution': '',\n",
    "            'meetingRecord': '',\n",
    "            'draftResolution': '',\n",
    "            'note': '',\n",
    "            'date': '',\n",
    "            'enPdfLink': '',\n",
    "            'collections': ''\n",
    "        }\n",
    "        container = souped_data.find('div', id='details-collapse').find_all('div', class_='metadata-row')\n",
    "        for data_row in container:\n",
    "            # check label of metadata for the kind of metadata in that row\n",
    "            match data_row.span.get_text().strip():\n",
    "                case 'Title':\n",
    "                    extracted_data['title'] = data_row.find_all('span')[1].get_text()\n",
    "                case 'Authors':\n",
    "                    extracted_data['authors'] = data_row.find_all('span')[1].get_text()\n",
    "                case 'Agenda information':\n",
    "                    extracted_data['agenda'] = data_row.find_all('span')[1].get_text()\n",
    "                case 'Description':\n",
    "                    extracted_data['description'] = data_row.find_all('span')[1].get_text()\n",
    "                case 'Resolution / Decision':\n",
    "                    extracted_data['resolution'] = data_row.find_all('span')[1].get_text()\n",
    "                case 'Meeting record':\n",
    "                    extracted_data['meetingRecord'] = data_row.find_all('span')[1].get_text()\n",
    "                case 'Draft resolution':\n",
    "                    extracted_data['draftResolution'] = data_row.find_all('span')[1].get_text()\n",
    "                case 'Note':\n",
    "                    extracted_data['note'] = data_row.find_all('span')[1].get_text()\n",
    "                case 'Vote date':\n",
    "                    extracted_data['date'] = data_row.find_all('span')[1].get_text()\n",
    "                case 'Date':\n",
    "                    extracted_data['date'] = data_row.find_all('span')[1].get_text()\n",
    "                case 'Collections':\n",
    "                    extracted_data['collections'] = data_row.find_all('span')[1].get_text()\n",
    "        return extracted_data\n",
    "    \n",
    "    def get_en_pdf_link(self, souped_data, *args, **kwargs):\n",
    "        \"\"\"gets all the PDF links on the page\"\"\"\n",
    "        link = ''\n",
    "        for pdf_row in souped_data.find_all('tr')[1:]:\n",
    "            if pdf_row.find_all('td')[4].get_text() == 'English':\n",
    "                link =  pdf_row.find_all('td')[0].find('tindui-app-file-download-link')['url']\n",
    "        return link\n",
    "    \n",
    "    @property\n",
    "    def pdf(self):\n",
    "        return PDFScraper(self.metadata['enPdfLink'], scrape_delay=self.scrape_delay).data\n",
    "\n",
    "scraper = UNDLArticleSeleniumScraper('https://digitallibrary.un.org/record/795883?v=pdf#files')\n",
    "print(scraper.pdf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ebola",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
