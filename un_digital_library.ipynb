{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the United Nation's Digital Library\n",
    "\n",
    "**Scraping script done on behalf of Oladoyin Okunoren @ Boston College**\n",
    "\n",
    "By David J. Thomas\n",
    "\n",
    "---\n",
    "\n",
    "This notebook contains a series of scripts to scrape every item from the United Nation's Digital Librarys database about Ebola from 2014-2016. It is a part of the dissertation of research of Oladoyin Okunoren, at [Boston College](https://bc.edu)\n",
    "\n",
    "---\n",
    "\n",
    "## Installation\n",
    "\n",
    "``` bash\n",
    "pip install -r requirements.txt\n",
    "jupyter lab\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Base Scraper Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "class BaseSeleniumScraper:\n",
    "    \"\"\"Base class for all Selenium Scraper objects.\"\"\"\n",
    "    url = ''\n",
    "    scrape_delay = 5\n",
    "    options = Options()\n",
    "    webdriver_path = '/usr/local/bin/chromedriver'\n",
    "    service = None\n",
    "    driver = None\n",
    "    metadata = {}\n",
    "    wait = None\n",
    "\n",
    "\n",
    "    def __init__(self, url,  *args, **kwargs):\n",
    "        # store the target url\n",
    "        self.url = url\n",
    "        # store scrape delay, if provided and a valid number\n",
    "        if 'scrape_delay' in kwargs:\n",
    "            if not isinstance(kwargs['scrape_delay'], int) and not isinstance(kwargs['scrape_delay'], float) and not isinstance(kwargs['scrape_delay'], complex):\n",
    "                raise Exception('Argument \\'scrape_delay must be an integer\\', float, or complex number')\n",
    "            self.scrape_delay = kwargs['scrape_delay']\n",
    "        # store webdriver_path, if a string and pointing to a file\n",
    "        if 'webdriver_path' in kwargs:\n",
    "            if not isinstance(kwargs['scrape_delay'], str):\n",
    "                raise Exception('Argument \\'webdriver_path\\' must be a string')\n",
    "            if not os.path.isfile(os.path.abspath(kwargs['webdriver_path'])):\n",
    "                raise Exception('Argument \\'webdriver_path\\' must point to a valid webdriver')\n",
    "            self.webdriver_path = kwargs['webdriver_path']\n",
    "        # comment out this line to run Chrome normally\n",
    "        self.options.add_argument(\"--headless\")\n",
    "        self.options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
    "        self.service = ChromeService(executable_path=self.webdriver_path)\n",
    "        self.driver = webdriver.Chrome(service=self.service, options=self.options)\n",
    "        self.wait = WebDriverWait(self.driver, self.scrape_delay)\n",
    "        try:\n",
    "            self.load()\n",
    "        except Exception as e:\n",
    "            self.shutdown()\n",
    "            raise Exception(e)\n",
    "        # make sure to shutdown the driver even if error occurs\n",
    "        try:\n",
    "            self.post_load()\n",
    "        except Exception as e:\n",
    "            self.shutdown()\n",
    "            raise Exception(e)\n",
    "        self.shutdown()\n",
    "\n",
    "    def load(self, *args, **kwargs):\n",
    "        \"\"\"Enforces a per-page scraping delay, then performs the inital page load.\"\"\"\n",
    "        print('Sleeping for', self.scrape_delay)\n",
    "        time.sleep(self.scrape_delay)\n",
    "        # fetch the page data\n",
    "        print('Getting page at ', self.url)\n",
    "        self.driver.get(self.url)\n",
    "\n",
    "    def post_load(self, *args, **kwargs):\n",
    "        \"\"\"Runs after the initial load of page data. Placeholder, SHOULD BE OVERWRITTEN by child classes to extract data\"\"\"\n",
    "        pass\n",
    "\n",
    "    def shutdown(self, *args, **kwargs):\n",
    "        self.driver.quit()\n",
    "\n",
    "print('Class defined, PROCEED.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Object to Scrape Browse Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNDLBrowseScraper(BaseSeleniumScraper):\n",
    "    data = []\n",
    "\n",
    "    def post_load(self):\n",
    "        self.data = self.gather_links()\n",
    "\n",
    "    @property\n",
    "    def next_link(self):\n",
    "        link = None\n",
    "        try:\n",
    "            navigation_container = self.wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, 'span.rec-navigation'))).get_attribute('innerHTML')\n",
    "        # if element isn't found, results were only a single page, so return None as there is no next link\n",
    "        except:\n",
    "            return None\n",
    "        souped_navigation_container = BeautifulSoup(navigation_container, 'html.parser')\n",
    "        link_containers = souped_navigation_container.find_all('a', class_='img')\n",
    "        for link_container in link_containers:\n",
    "            if link_container.img['alt'] == 'next':\n",
    "                link = link_container['href']\n",
    "        if link is None:\n",
    "            return None\n",
    "        return 'https://digitallibrary.un.org' + link\n",
    "\n",
    "    @property\n",
    "    def links(self):\n",
    "        \"\"\"Gets the links to the article on the page as it currently exists\"\"\"\n",
    "        links = []\n",
    "        link_container = self.driver.find_element(By.CSS_SELECTOR, 'form.all-results').get_attribute('innerHTML')\n",
    "        container_soup = BeautifulSoup(link_container, 'html.parser')\n",
    "        for result_row in container_soup.find('table').find_all('tr'):\n",
    "            article_link = 'https://digitallibrary.un.org' + result_row.find('div', class_='result-title').a['href'] + '&v=pdf'\n",
    "            links.append(article_link)\n",
    "        return links\n",
    "    \n",
    "    def gather_links(self):\n",
    "        \"\"\"Recursive function to gather links to all articles on this page, and subsequent pages. If not on the last page,\n",
    "        return the links on the page plus those returned by a recursively call another UNDLBrowseScraper object on the next page.\n",
    "        If on the last page, just return the links and break the recursive loop\"\"\"\n",
    "        # if no next_link, just return the links on the page\n",
    "        if not self.next_link:\n",
    "            return self.links\n",
    "        else:\n",
    "            return self.links + UNDLBrowseScraper(self.next_link, scrape_delay=self.scrape_delay).data\n",
    "\n",
    "article_links = []\n",
    "search_urls = [\n",
    "    # 2014 articles\n",
    "    'https://digitallibrary.un.org/search?ln=en&p=ebola&f=&rm=&sf=&so=d&rg=50&c=Resource%20Type&c=UN%20Bodies&c=&of=hb&fti=1&fct__3=2016&fti=1',\n",
    "    # 2016 articles\n",
    "    'https://digitallibrary.un.org/search?ln=en&p=ebola&f=&rm=&sf=&so=d&rg=50&c=Resource%20Type&c=UN%20Bodies&c=&of=hb&fti=1&fct__3=2014&fti=1',\n",
    "    # 2015 articles were too numerous, had to break into several search queries by UN Body to reduce to < 500\n",
    "    'https://digitallibrary.un.org/search?ln=en&p=ebola&f=&rm=&sf=&so=d&rg=50&c=Resource%20Type&c=UN%20Bodies&c=&of=hb&fti=1&fct__3=2015&fct__2=General%20Assembly&fti=1',\n",
    "    'https://digitallibrary.un.org/search?ln=en&p=ebola&f=&rm=&sf=&so=d&rg=50&c=Resource%20Type&c=UN%20Bodies&c=&of=hb&fti=1&fct__3=2015&fct__2=Security%20Council&fti=1',\n",
    "    'https://digitallibrary.un.org/search?ln=en&p=ebola&f=&rm=&sf=&so=d&rg=50&c=Resource%20Type&c=UN%20Bodies&c=&of=hb&fti=1&fct__3=2015&fct__2=Economic%20and%20Social%20Council&fti=1',\n",
    "    'https://digitallibrary.un.org/search?ln=en&p=ebola&f=&rm=&sf=&so=d&rg=50&c=Resource%20Type&c=UN%20Bodies&c=&of=hb&fti=1&fct__3=2015&fct__2=Human%20Rights%20Bodies&fti=1',\n",
    "    'https://digitallibrary.un.org/search?ln=en&p=ebola&f=&rm=&sf=&so=d&rg=50&c=Resource%20Type&c=UN%20Bodies&c=&of=hb&fti=1&fct__3=2015&fct__2=Programmes%20and%20Funds&fti=1',\n",
    "    'https://digitallibrary.un.org/search?ln=en&p=ebola&f=&rm=&sf=&so=d&rg=50&c=Resource%20Type&c=UN%20Bodies&c=&of=hb&fti=1&fct__3=2015&fct__2=Other%20UN%20Bodies%20and%20Entities&fti=1',\n",
    "    'https://digitallibrary.un.org/search?ln=en&p=ebola&f=&rm=&sf=&so=d&rg=50&c=Resource%20Type&c=UN%20Bodies&c=&of=hb&fti=1&fct__3=2015&fct__2=Secretariat&fti=1',\n",
    "    'https://digitallibrary.un.org/search?ln=en&p=ebola&f=&rm=&sf=&so=d&rg=50&c=Resource%20Type&c=UN%20Bodies&c=&of=hb&fti=1&fct__3=2015&fct__2=Economic%20Commissions&fti=1'\n",
    "]\n",
    "for search_url in search_urls:\n",
    "    scraper = UNDLBrowseScraper(search_url, scrape_delay=30)\n",
    "    article_links += scraper.data\n",
    "\n",
    "# remove any duplicate links\n",
    "article_links = list(set(article_links))\n",
    "\n",
    "print('Success! Gathered', len(article_links), 'articles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a PDF Scraper Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import fitz\n",
    "\n",
    "class PDFScraper:\n",
    "    \"\"\"Separate scraper to handle fetching/converting the PDF into text. Will be used by the UNDLArticleScraper below\"\"\"\n",
    "    url = ''\n",
    "    scrape_delay = 5\n",
    "    max_tries = 5\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0\",\n",
    "            \"Accept-Encoding\": \"*\",\n",
    "            \"Connection\": \"keep-alive\"\n",
    "        }\n",
    "    data = None\n",
    "\n",
    "    def __init__(self, url, *args, **kwargs):\n",
    "        if type(url) != str:\n",
    "            raise Exception('URL must be a valid string')\n",
    "        self.url = url\n",
    "        # store scrape delay, if provided and a valid number\n",
    "        if 'scrape_delay' in kwargs:\n",
    "            if not isinstance(kwargs['scrape_delay'], int) and not isinstance(kwargs['scrape_delay'], float) and not isinstance(kwargs['scrape_delay'], complex):\n",
    "                raise Exception('Argument \\'scrape_delay must be an integer\\', float, or complex number')\n",
    "            self.scrape_delay = kwargs['scrape_delay']\n",
    "        # store scrape delay, if provided and a valid number\n",
    "        if 'max_tries' in kwargs:\n",
    "            if isinstance(kwargs['max_tries'], int) or isinstance(kwargs['max_tries'], float) or isinstance(kwargs['max_tries'], complex):\n",
    "                raise Exception('Argument \\'scrape_delay must be an integer\\', float, or complex number')\n",
    "            self.max_tries = kwargs['max_tries']\n",
    "        self.data = self.pdf\n",
    "\n",
    "    def get_pdf_data(self, tries_left=5):\n",
    "        \"\"\"Fetch data and returns raw content... if fail to fetch, returns self recursively, with tries_left decremented\"\"\"\n",
    "        # enforce scrape delay\n",
    "        pdf_content = None\n",
    "        time.sleep(self.scrape_delay)\n",
    "        if not tries_left or type(tries_left) != int or tries_left < 0:\n",
    "            tries_left = 5\n",
    "        print('Getting PDF at ', self.url)\n",
    "        # attempt to get page data, decrement tries_left if successful\n",
    "        try:\n",
    "            pdf_content = requests.get(self.url, headers=self.headers).content\n",
    "            tries_left -= 1\n",
    "         # if an error occured, retry by returning recursively\n",
    "        except:\n",
    "            print('Error getting', self.url)\n",
    "            if tries_left > 0:\n",
    "                print('Retrying...')\n",
    "                return self.get_pdf_data(self.url, tries_left=tries_left-1)\n",
    "            if tries_left <= 0:\n",
    "                print('Retry limit reached, ABORTING parse of', self.url)\n",
    "                return None\n",
    "        return pdf_content\n",
    "    \n",
    "    @property\n",
    "    def pdf(self):\n",
    "        \"\"\"Returns the content of the PDF as text\"\"\"\n",
    "        text = ''\n",
    "        pdf_document = fitz.open('pdf', self.get_pdf_data(self.max_tries))\n",
    "        for page_num in range(pdf_document.page_count):\n",
    "            page = pdf_document[page_num]\n",
    "            text += page.get_text()\n",
    "        # return text with redundant whitespace and all newlines/tabs replaced\n",
    "        text = text.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ')\n",
    "        text = ' '.join(text.split())\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Scraper Object for Individual Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class UNDLArticleScraper(BaseSeleniumScraper):\n",
    "    \"\"\"Used to scrape the metadata & pdf link from a single article page. Uses the PDF scraper to get contents of the PDF\"\"\"\n",
    "\n",
    "    def post_load(self, *args, **kwargs):\n",
    "        metadata_container = self.wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, 'div.detailed-record-content'))).get_attribute('innerHTML')\n",
    "        time.sleep(self.scrape_delay)\n",
    "        metadata_soup = BeautifulSoup(metadata_container, 'html.parser')\n",
    "        self.metadata = self.get_metadata(metadata_soup)\n",
    "        try:\n",
    "            files_container = self.wait.until(EC.visibility_of_element_located((By.ID, 'record-files-list'))).get_attribute('innerHTML')\n",
    "            files_soup = BeautifulSoup(files_container, 'html.parser')\n",
    "            self.metadata['enPdfLink'] = self.get_en_pdf_link(files_soup)\n",
    "        except:\n",
    "            self.metadata['enPdfLink'] = ''\n",
    "\n",
    "    def get_metadata(self, souped_data, *args, **kwargs):\n",
    "        \"\"\"Extract metadata from page after load and return as a dictionary\"\"\"\n",
    "        extracted_data = {\n",
    "            'title': '',\n",
    "            'authors': '',\n",
    "            'description': '',\n",
    "            'agenda': '',\n",
    "            'resolution': '',\n",
    "            'meetingRecord': '',\n",
    "            'draftResolution': '',\n",
    "            'note': '',\n",
    "            'date': '',\n",
    "            'enPdfLink': '',\n",
    "            'url': self.url,\n",
    "            'collections': ''\n",
    "        }\n",
    "        container = souped_data.find('div', id='details-collapse').find_all('div', class_='metadata-row')\n",
    "        for data_row in container:\n",
    "            # check label of metadata for the kind of metadata in that row\n",
    "            match data_row.span.get_text().strip():\n",
    "                case 'Title':\n",
    "                    extracted_data['title'] = data_row.find_all('span')[1].get_text()\n",
    "                case 'Authors':\n",
    "                    extracted_data['authors'] = data_row.find_all('span')[1].get_text()\n",
    "                case 'Agenda information':\n",
    "                    extracted_data['agenda'] = data_row.find_all('span')[1].get_text()\n",
    "                case 'Description':\n",
    "                    extracted_data['description'] = data_row.find_all('span')[1].get_text()\n",
    "                case 'Resolution / Decision':\n",
    "                    extracted_data['resolution'] = data_row.find_all('span')[1].get_text()\n",
    "                case 'Meeting record':\n",
    "                    extracted_data['meetingRecord'] = data_row.find_all('span')[1].get_text()\n",
    "                case 'Draft resolution':\n",
    "                    extracted_data['draftResolution'] = data_row.find_all('span')[1].get_text()\n",
    "                case 'Note':\n",
    "                    extracted_data['note'] = data_row.find_all('span')[1].get_text()\n",
    "                case 'Vote date':\n",
    "                    extracted_data['date'] = data_row.find_all('span')[1].get_text()\n",
    "                case 'Date':\n",
    "                    extracted_data['date'] = data_row.find_all('span')[1].get_text()\n",
    "                case 'Collections':\n",
    "                    extracted_data['collections'] = data_row.find_all('span')[1].get_text()\n",
    "        return extracted_data\n",
    "    \n",
    "    def get_en_pdf_link(self, souped_data, *args, **kwargs):\n",
    "        \"\"\"gets all the PDF links on the page\"\"\"\n",
    "        link = ''\n",
    "        for pdf_row in souped_data.find_all('tr')[1:]:\n",
    "            if pdf_row.find_all('td')[4].get_text() == 'English':\n",
    "                link =  pdf_row.find_all('td')[0].find('tindui-app-file-download-link')['url']\n",
    "        return link\n",
    "    \n",
    "    @property\n",
    "    def pdf(self):\n",
    "        if self.metadata['enPdfLink'] != '':\n",
    "            return PDFScraper(self.metadata['enPdfLink'], scrape_delay=self.scrape_delay).data\n",
    "        else:\n",
    "            return ''\n",
    "    \n",
    "story_data = []\n",
    "\n",
    "counter = 0\n",
    "for article_link in article_links:\n",
    "    counter += 1\n",
    "    print('Getting article', counter, 'of', len(article_links))\n",
    "    story_datum = {}\n",
    "    scraper = UNDLArticleScraper(article_link)\n",
    "    story_datum = scraper.metadata\n",
    "    story_datum['text'] = scraper.pdf\n",
    "    # ignore blank stories (where there is no text, because the file is not online)\n",
    "    if story_datum['text'] != '':\n",
    "        story_data.append(story_datum)\n",
    "\n",
    "print('Scraped', len(article_links), 'succesfully!')\n",
    "print(story_data[0:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving to File (CSV)\n",
    "\n",
    "Now we need to output the data for text analysis. In this step we will output each record as a line in a .CSV (spreadsheet) file. That file will be stored in `output/un_digital_library.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "OUTPUT_CSV_FILENAME = 'un_digital_library_3.csv'\n",
    "OUTPUT_CSV_FIELDNAMES = ['title', 'authors', 'description', 'agenda', 'resolution', 'meetingRecord', 'draftResolution', 'note', 'date', 'enPdfLink', 'url', 'collections', 'text']\n",
    "\n",
    "output_filepath = os.path.join(os.path.abspath(os.getcwd()), 'output', OUTPUT_CSV_FILENAME)\n",
    "\n",
    "# ensure directory exists, if not, create it\n",
    "if not os.path.exists(os.path.join(os.path.abspath(os.getcwd()), 'output')):\n",
    "    os.makedirs(os.path.join(os.path.abspath(os.getcwd()), 'output'))\n",
    "\n",
    "print('Writing CSV File ', output_filepath)\n",
    "with open(output_filepath, 'w+', encoding='utf8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=OUTPUT_CSV_FIELDNAMES)\n",
    "    writer.writeheader()\n",
    "    for story_datum in story_data:\n",
    "        writer.writerow(story_datum)\n",
    "\n",
    "print('Success writing CSV File!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving to File (TXT)\n",
    "\n",
    "Finally, some text analysis packages use folders of .txt files, instead of .csv files. So, we will also output every record as a .txt file that will be located inside of `output/un_digital_library/FILENAME.txt`, where the FILENAME will be determined by the english PDF link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OUTPUT_FOLDERNAME = 'un_digital_library'\n",
    "\n",
    "output_folderpath = os.path.join(os.path.abspath(os.getcwd()), 'output', OUTPUT_FOLDERNAME)\n",
    "\n",
    "# ensure directory exists, if not, create it\n",
    "if not os.path.exists(output_folderpath):\n",
    "    os.makedirs(output_folderpath)\n",
    "\n",
    "print('Writing TXT Files ', output_folderpath)\n",
    "for story_datum in story_data:\n",
    "    # get the filename from the last segment of the PDF link, remove the 'en' suffix and the file type and replace with .txt\n",
    "    output_filename = story_datum['enPdfLink'].split('/')[-1].replace('-EN.pdf', '.txt')\n",
    "    output_filepath = os.path.join(output_folderpath, output_filename)\n",
    "    txtfile = open(output_filepath, 'w+', encoding='utf8')\n",
    "    txtfile.write(story_datum['text'])\n",
    "    txtfile.close()\n",
    "\n",
    "print('Success writing TXT Files!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ebola",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
