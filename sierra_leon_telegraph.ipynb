{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the Sierra Leon Telegraph\n",
    "\n",
    "**Scraping script done on behalf of Oladoyin Okunoren @ Boston College**\n",
    "\n",
    "By David J. Thomas\n",
    "\n",
    "---\n",
    "\n",
    "This notebook contains a series of scripts to scrape every news story from the Sierra Leone Telegraph about Ebola from 2014-2016. It is a part of the dissertation of research of Oladoyin Okunoren, at [Boston College](https://bc.edu)\n",
    "\n",
    "---\n",
    "\n",
    "## Installation\n",
    "\n",
    "``` bash\n",
    "pip install -r requirements.txt\n",
    "jupyter lab\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Links to Stories\n",
    "\n",
    "This first step attempts to go to the url for search results for each of the three years in question (2014-2016). On each page, it will wait until the modal-popover appears which contains the links for each story. However... this popover is an infinite scroll that doens't load all the stories at once.\n",
    "\n",
    "So, we will have to use the Selenium webdriver to run Chrome headlessly (i.e. in the background). For the results for each year, we will call the `gather_links()` function. It will need to use Selenium to automatically scroll the page over and over... until it detects that no new results have been loaded after scrolling. It will do this by using the `scrollIntoView()` method of Selenium, and targetting an element that is at the footer of the list. This will cause the page to scroll to the bottom. It will then check the height (in pixels) of the container element holding the stories. If that height has not changed since before the scroll, no new results were loaded and scrolling can stop.\n",
    "\n",
    "At that point, once no more results load, we will use BeautifulSoup to parse the results, extract the URL of every story... and `gather_links()` will return them as a list of links.\n",
    "\n",
    "Below the function definitions, we actually call `gather_links()` on each year's URL, and aggregate them in a variable called `story_links` which we will use in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "SCAPE_DELAY = 5\n",
    "   \n",
    "def scroll_popover(element_css_selector, driver):\n",
    "    \"\"\"Scrolls the page down to the given CSS selector\"\"\"\n",
    "    element = False\n",
    "    try:\n",
    "        element = driver.find_element(By.CSS_SELECTOR, element_css_selector)\n",
    "    except:\n",
    "        element = False\n",
    "    if element:\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", element)\n",
    "\n",
    "def gather_browse_links(url):\n",
    "    \"\"\"Uses Selenium to load a search results page, and scroll infinitely until there are no more results... then gather and return a list of links to stories\"\"\"\n",
    "    # links to be returned\n",
    "    links = []\n",
    "    # initialize webdriver\n",
    "    webdriver_path = '/usr/local/bin/chromedriver'\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_service = ChromeService(executable_path=webdriver_path)\n",
    "    driver = webdriver.Chrome(service=chrome_service, options=chrome_options)\n",
    "    # open the browse page and wait for popover to load\n",
    "    driver.get(url)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    pop_over = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div.jetpack-instant-search__search-results-content')))\n",
    "    # make sure popover is in view\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView(true);\", pop_over)\n",
    "    # keep scrolling as long as height hasn't changed\n",
    "    previous_height = driver.execute_script(\"return arguments[0].scrollHeight\", pop_over)\n",
    "    while True:\n",
    "        # scroll popover to the bottom pagination element\n",
    "        scroll_popover('div.jetpack-instant-search__search-results-pagination', driver)\n",
    "        time.sleep(SCAPE_DELAY)\n",
    "        # see if height has changed\n",
    "        new_height = driver.execute_script(\"return arguments[0].scrollHeight\", pop_over)\n",
    "        if new_height == previous_height:\n",
    "            break\n",
    "        previous_height = new_height\n",
    "\n",
    "    # once no more results, get the full html of the results. Parse with Sooup, and break into individual results\n",
    "    pop_over_html = pop_over.get_attribute('innerHTML')\n",
    "    soup = BeautifulSoup(pop_over_html, 'html.parser')\n",
    "    stories = soup.find_all('li', class_='jetpack-instant-search__search-result')\n",
    "    # iterate over results\n",
    "    for story in stories:\n",
    "        if story.find('a'):\n",
    "            links.append('http://' + str(story.find('a')['href'])[2:])\n",
    "    # terminate browser\n",
    "    driver.quit()\n",
    "    return links\n",
    "\n",
    "target_urls = [\n",
    "    'https://www.thesierraleonetelegraph.com/?s=ebola&year_post_date=2014-01-01%2000%3A00%3A00',\n",
    "    'https://www.thesierraleonetelegraph.com/?s=ebola&year_post_date=2015-01-01%2000%3A00%3A00',\n",
    "    'https://www.thesierraleonetelegraph.com/?s=ebola&year_post_date=2016-01-01%2000%3A00%3A00'\n",
    "]\n",
    "\n",
    "story_links = []\n",
    "\n",
    "for target_url in target_urls:\n",
    "    story_links += gather_browse_links(target_url)\n",
    "\n",
    "print((str(len(story_links))) + ' stories')\n",
    "\n",
    "for link in story_links[0:5]:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Individual Story Pages (Will Take Swhile)\n",
    "\n",
    "Since individual story pages do not require scrolling, or any other kind of dynamic interaction, we don't need to use as complex a solution as Selenium for this step. In our function to scrape each page `soup_page()`, we will use the `requests` library, to make simple HTTP requests to a URL and fetch the HTML. Then, we can parse it with `BeautifulSoup`, similar to the step above. `scrape_page()` will use the parsed HTML from the `soup_page()` function to extract the specific bits of destired data (text & metadata).\n",
    "\n",
    "Below our function definitions, we then call the `scrape_page()` function on every `story_link` that we gathered in the step above. Each extracted record it added to a list of dictionaries in `story_data`. We will then save that data in the steps that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def soup_page(url, scrape_delay=10, tries_left = 5):\n",
    "    \"\"\"Receives URL, requests raw html, then returns converted BeautifulSoup object.\"\"\"\n",
    "    # declare variable for raw html\n",
    "    page_html = None\n",
    "    # ensure tries_left is set and valid, if not set to 5, check if url is valid\n",
    "    if not tries_left or type(tries_left) != int or tries_left < 0:\n",
    "        tries_left = 5\n",
    "    if type(url) != str:\n",
    "        raise Exception('URL must be a valid string')\n",
    "    # enforce a time delay between each scrape for good internet citizenship\n",
    "    time.sleep(scrape_delay)\n",
    "    print('Getting', url)\n",
    "    # attempt to get page data, decrement tries_left if successful\n",
    "    try:\n",
    "        page_html = requests.get(url).text\n",
    "        tries_left -= 1\n",
    "    # if an error occured, retry by returning recursively\n",
    "    except:\n",
    "        print('Error getting', url)\n",
    "        if tries_left > 0:\n",
    "            print('Retrying...')\n",
    "            return soup_page(url, tries_left=tries_left-1)\n",
    "        if tries_left <= 0:\n",
    "            print('Retry limit reached, ABORTING parse of', url)\n",
    "            return None\n",
    "    print('Success, souping...')\n",
    "    # if all went well, return new BeautifulSoup populated with the page html and parser set to html.parser\n",
    "    return BeautifulSoup(page_html, 'html.parser')\n",
    "\n",
    "def scrape_page(url):\n",
    "    \"\"\"Receives a url, uses soup_page to fetch/parse it as soup.\n",
    "    This function extracts specific desired data (text & metadata)\n",
    "    and returns it in a dictionary\"\"\"\n",
    "    extracted_data = {\n",
    "        'title': '',\n",
    "        'date': '',\n",
    "        'author': '',\n",
    "        'column': '',\n",
    "        'link': url,\n",
    "        'publication': 'The Sierra Leone Telegraph',\n",
    "        'type': 'Local',\n",
    "        'text': ''\n",
    "    }\n",
    "    soup = soup_page(url)\n",
    "    # if page was not fetched successfully, abort and return None\n",
    "    if not soup:\n",
    "        return None\n",
    "    # get specific bits of metadata\n",
    "    extracted_data['title'] = soup.find('h1', class_='entry-title').get_text()\n",
    "    extracted_data['date'] = soup.find('span', class_='entry-meta-date').get_text()\n",
    "    extracted_data['author'] = soup.find('span', class_='entry-meta-author').get_text()\n",
    "    extracted_data['column'] = soup.find('span', class_='entry-meta-categories').get_text()\n",
    "    extracted_data['text'] = ' '.join(soup.find('div', class_='entry-content').get_text().split())\n",
    "    return extracted_data\n",
    "\n",
    "# iterate each link to a story, extract data, and append to story_data\n",
    "story_data = []\n",
    "counter = 0\n",
    "for link in story_links:\n",
    "    counter += 1\n",
    "    print(str(counter) + '/' + str(len(story_links)))\n",
    "    story_data.append(scrape_page(link))\n",
    "\n",
    "for story_datum in story_data[0:5]:\n",
    "    print(story_datum)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving to File (CSV)\n",
    "\n",
    "Now we need to output the data for text analysis. In this step we will output each record as a line in a .CSV (spreadsheet) file. That file will be stored in `output/awoko_newspaper.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "OUTPUT_CSV_FILENAME = 'sierra_leon_telegraph.csv'\n",
    "OUTPUT_CSV_FIELDNAMES = ['title', 'date', 'author', 'column', 'publication', 'link', 'type', 'text']\n",
    "\n",
    "output_filepath = os.path.join(os.path.abspath(os.getcwd()), 'output', OUTPUT_CSV_FILENAME)\n",
    "\n",
    "# ensure directory exists, if not, create it\n",
    "if not os.path.exists(os.path.join(os.path.abspath(os.getcwd()), 'output')):\n",
    "    os.makedirs(os.path.join(os.path.abspath(os.getcwd()), 'output'))\n",
    "\n",
    "print('Writing CSV File ', output_filepath)\n",
    "with open(output_filepath, 'w+', encoding='utf8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=OUTPUT_CSV_FIELDNAMES)\n",
    "    writer.writeheader()\n",
    "    for story_datum in story_data:\n",
    "        writer.writerow(story_datum)\n",
    "\n",
    "print('Success writing CSV File!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving to File (TXT)\n",
    "\n",
    "\n",
    "Finally, some text analysis packages use folders of .txt files, instead of .csv files. So, we will also output every record as a .txt file that will be located inside of `output/awoko_newspaper/FILENAME.txt`, where the FILENAME will be determined by the url to the story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OUTPUT_FOLDERNAME = 'sierra_leon_telegraph'\n",
    "\n",
    "output_folderpath = os.path.join(os.path.abspath(os.getcwd()), 'output', OUTPUT_FOLDERNAME)\n",
    "\n",
    "# ensure directory exists, if not, create it\n",
    "if not os.path.exists(output_folderpath):\n",
    "    os.makedirs(output_folderpath)\n",
    "\n",
    "print('Writing TXT Files ', output_folderpath)\n",
    "for story_datum in story_data:\n",
    "    output_filename = story_datum['link'].split('/')[-2] + '.txt'\n",
    "    output_filepath = os.path.join(output_folderpath, output_filename)\n",
    "    txtfile = open(output_filepath, 'w+', encoding='utf8')\n",
    "    txtfile.write(story_datum['text'])\n",
    "    txtfile.close()\n",
    "\n",
    "print('Success writing TXT Files!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
