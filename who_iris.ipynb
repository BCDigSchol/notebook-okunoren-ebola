{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the World Health Organization's Iris (Institutional Repository for Information Sharing) Database\n",
    "\n",
    "**Scraping script done on behalf of Oladoyin Okunoren @ Boston College**\n",
    "\n",
    "By David J. Thomas\n",
    "Edited by Lester E. Carver\n",
    "\n",
    "---\n",
    "\n",
    "This notebook contains a series of scripts to scrape every news story from the Awoko Newspaper about Ebola from 2014-2016. It is a part of the dissertation of research of Oladoyin Okunoren, at [Boston College](https://bc.edu)\n",
    "\n",
    "---\n",
    "\n",
    "## Installation\n",
    "\n",
    "``` bash\n",
    "pip install -r requirements.txt\n",
    "jupyter lab\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining A General Object to Scrape Any Page\n",
    "\n",
    "Before we can get going scraping pages, this step below will create a general class called `BasePageScraper`, with a few basic functions that, when given a URL, will fetch the page's HTML, and then parse it using BeautifulSoup. This object won't be used directly. However, it will serve as a base class for objects defined in the following steps that are aimed at scraping specific pages. These child classes will pick up the methods and attributes of their parents. Once this base class is defined, we will print a success message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import fitz\n",
    "from urllib.parse import urlparse, unquote\n",
    "import re\n",
    "import csv\n",
    "\n",
    "class BasePageScraper:\n",
    "    \"\"\"Gets HTML for a single page, parses it with BeautifulSoup, and stores results in self.data.\n",
    "    Base class for child classes which target specific pages\"\"\"\n",
    "\n",
    "    def __init__(self, url, total_tries=5, scrape_delay=5):\n",
    "        \"\"\"Gets url and pre-parses the html\"\"\"\n",
    "        self._url = url\n",
    "        self._total_tries = total_tries\n",
    "        self._scrape_delay = scrape_delay\n",
    "        # store souped data in object for extraction\n",
    "        self.data = self.soup_page(tries_left=self._total_tries)\n",
    "\n",
    "    def soup_page(self, tries_left=5):\n",
    "        \"\"\"Receives URL, requests raw html, then returns converted BeautifulSoup object.\"\"\"\n",
    "        # declare variable for raw html\n",
    "        page_html = None\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0\",\n",
    "            \"Accept-Encoding\": \"*\",\n",
    "            \"Connection\": \"keep-alive\"\n",
    "        }\n",
    "        # ensure tries_left is set and valid, if not set to 5, check if url is valid\n",
    "        if not tries_left or type(tries_left) != int or tries_left < 0:\n",
    "            tries_left = 5\n",
    "        if type(self._url) != str:\n",
    "            raise Exception('URL must be a valid string')\n",
    "        # enforce a time delay between each scrape for good internet citizenship\n",
    "        time.sleep(self._scrape_delay)\n",
    "        print('Getting', self._url)\n",
    "        # attempt to get page data, decrement tries_left if successful\n",
    "        try:\n",
    "            page_html = requests.get(self._url, headers=headers).text\n",
    "            tries_left -= 1\n",
    "        # if an error occured, retry by returning recursively\n",
    "        except:\n",
    "            print('Error getting', self._url)\n",
    "            if tries_left > 0:\n",
    "                print('Retrying...')\n",
    "                print(tries_left)\n",
    "                return self.soup_page(self._url, tries_left=tries_left-1)\n",
    "            if tries_left <= 0:\n",
    "                print('Retry limit reached, ABORTING parse of', self._url)\n",
    "                return None\n",
    "        print('Success, souping...')\n",
    "        # if all went well, return new BeautifulSoup populated with the page html and parser set to html.parser\n",
    "        return BeautifulSoup(page_html, 'html.parser')\n",
    "\n",
    "print('Function defined! PROCEED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the WHO IRIS Browse Page (Will take awhile)\n",
    "\n",
    "This step will find the links to each story about ebola, from 2014-2016 onwards. The scraping object defined below uses the parent class from above. Howevr, it adds a few extra functions. One gets the link from the \"next button (`.next_link`), if it exists. Another function gives a list of every link to an article on that specific browse page (`.links`). The last function, `.gather_links()`, gets all the links from the given page... and then calls itself reccursively on every following page (using the next page links) to get every link... not just those on this page, but on all following pages. All of those links are gathered together in a variable called `article_links` which will be used in the next step. There, we will use another scraper object on each of those pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRISBrowseScraper(BasePageScraper):\n",
    "    \"\"\"Represents a browse page of the WHO IRIS Database. Gathers links of stories returned from a search\"\"\"\n",
    "\n",
    "    def __init__(self, url, total_tries=5, scrape_delay=5):\n",
    "        super().__init__(url=url, total_tries=total_tries, scrape_delay=scrape_delay)\n",
    "\n",
    "    @property\n",
    "    def next_link(self):\n",
    "        \"\"\"Returns link to the next browse page if exists, or None if no further page\"\"\"\n",
    "        link = self.data.find('a', class_='next-page-link')\n",
    "        if not link:\n",
    "            return None\n",
    "        else:\n",
    "            return 'https://iris.who.int/' + link['href']\n",
    "\n",
    "    @property\n",
    "    def links(self):\n",
    "        \"\"\"Peruses the souped data and returns list of strings, each with a link\"\"\"\n",
    "        return_links = []\n",
    "        for article in self.data.find_all('div', class_='artifact-description'):\n",
    "            # append link of article to list\n",
    "            return_links.append('https://iris.who.int' + article.find('h4', class_='artifact-title').find('a')['href'])\n",
    "        return return_links\n",
    "\n",
    "    def gather_links(self):\n",
    "        \"\"\"Recursive function to gather links to all stories on this page, and subsequent pages. If not on the last page,\n",
    "        return the links on the page plus those returned by a recursively call another IRISBrowseScraper object on the next page.\n",
    "        If on the last page, just return the links and break the recursive loop\"\"\"\n",
    "        # if no next_link, just return the links on the page\n",
    "        if not self.next_link:\n",
    "            return self.links\n",
    "        else:\n",
    "            return self.links + IRISBrowseScraper(self.next_link).gather_links()\n",
    "\n",
    "article_links = []\n",
    "print('Gathering stories, this may take awhile...')\n",
    "url = 'https://iris.who.int/discover?search-result=true&query=ebola&scope=&filtertype_0=dateIssued&filtertype_1=iso&filter_relational_operator_1=equals&filter_relational_operator_0=equals&filter_1=English&filter_0=%5B2013+TO+2016%5D&rpp=100&sort_by=score&order=desc'\n",
    "article_links += IRISBrowseScraper(url).gather_links()\n",
    "\n",
    "\n",
    "print(str(len(article_links)) + ' links to stories gathered')\n",
    "print(article_links[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Each Article (Will take a long while)\n",
    "\n",
    "Warning: this step will take a long time to run. First, we will define another scraper object, this one representing the page of a document on the WHO IRIS site. Once we define the object, we will the use it on each url in `article_links` gathered above. The scraper object will make it quick and easy to extract metadata and links to the pdfs.\n",
    "\n",
    "Once we have defined the object, we will loop through each of the `article_links`, calling the object, and appending all of the data it extracts as a dictionary inside of the list `story_data`. In the next steps, we will download and scrape the pdfs for their text in the next steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRISArticleScraper(BasePageScraper):\n",
    "    \"\"\"Represents an article of the WHO. Aids extracting text and metadata\"\"\"\n",
    "\n",
    "    def __init__(self, url, total_tries=5, scrape_delay=5):\n",
    "        super().__init__(url=url, total_tries=total_tries, scrape_delay=scrape_delay)\n",
    "        self.data = self.data.find('div', class_='row')\n",
    "\n",
    "    @property\n",
    "    def title(self):\n",
    "        return self.data.find('span', id ='citation-article-title').get_text()\n",
    "\n",
    "    @property\n",
    "    def author(self):\n",
    "        outer_span = self.data.find('span', id='citation-article-authors')\n",
    "        if outer_span:\n",
    "            nested_span = outer_span.find('span')\n",
    "            if nested_span:\n",
    "                return nested_span.get_text()\n",
    "            else:\n",
    "                return \"No author found\"\n",
    "        else:\n",
    "            return \"No author found\"\n",
    "\n",
    "    @property\n",
    "    def date(self):\n",
    "        raw_date = self.data.find('span', id ='citation-article-date').get_text()\n",
    "        cleaned_date = re.sub(r'[^\\d]', '', raw_date)  # Remove non-numeric characters\n",
    "        return cleaned_date\n",
    "\n",
    "    @property\n",
    "    def publication(self):\n",
    "        return 'World Health Organization'\n",
    "\n",
    "    @property\n",
    "    def link(self):\n",
    "        return self._url\n",
    "\n",
    "    @property\n",
    "    def pdf(self):\n",
    "        outer_div = self.data.find('div', class_='item-page-field-wrapper table word-break')\n",
    "        if outer_div:\n",
    "            nested_div = outer_div.find('a')\n",
    "            if nested_div and 'href' in nested_div.attrs:\n",
    "                return 'https://iris.who.int' + nested_div['href']\n",
    "        return 'No pdf found'\n",
    "\n",
    "# loop through every link, create an IRISArtricleScraper object for it, extract data, and store in story_data\n",
    "story_data = []\n",
    "counter = 0\n",
    "for article_link in article_links:\n",
    "    counter += 1\n",
    "    print('Scraping article ' + str(counter) + '/' + str(len(article_links)))\n",
    "    current_page_scraper = IRISArticleScraper(article_link)\n",
    "    story_data.append({\n",
    "        'title': current_page_scraper.title,\n",
    "        'author': current_page_scraper.author,\n",
    "        'date': current_page_scraper.date,\n",
    "        'publication': current_page_scraper.publication,\n",
    "        'link': current_page_scraper.link,\n",
    "        'type': 'International',\n",
    "        'pdf': current_page_scraper.pdf\n",
    "    })\n",
    "\n",
    "print(story_data[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and Scraping PDF Text\n",
    "\n",
    "This step below will create a class called `PDFScraped`, with functions for downloading pdfs from each article and then extracting the text of each pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDFScraper:\n",
    "    \"\"\"Handles downloading and optionally extracting text from PDF files.\"\"\"\n",
    "\n",
    "    def __init__(self, save_path='pdfs'):\n",
    "        self.save_path = save_path\n",
    "        if not os.path.exists(self.save_path):\n",
    "            os.makedirs(self.save_path)\n",
    "\n",
    "    def download_pdf(self, pdf_url):\n",
    "        \"\"\"Download PDF from the given URL and save it to the specified directory.\"\"\"\n",
    "        parsed_url = urlparse(pdf_url)\n",
    "        pdf_filename = os.path.basename(parsed_url.path)\n",
    "        pdf_filename = unquote(pdf_filename)  # Decode URL-encoded filename\n",
    "        pdf_filename = os.path.join(self.save_path, pdf_filename)\n",
    "\n",
    "        try:\n",
    "            response = requests.get(pdf_url)\n",
    "            response.raise_for_status()\n",
    "            with open(pdf_filename, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            print(f'Successfully downloaded {pdf_filename}')\n",
    "            return pdf_filename\n",
    "        except requests.RequestException as e:\n",
    "            print(f'Failed to download {pdf_url}: {e}')\n",
    "            return None\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        \"\"\"Extract text from the given PDF file using PyMuPDF (fitz).\"\"\"\n",
    "        try:\n",
    "            document = fitz.open(pdf_path)\n",
    "            text = \"\"\n",
    "            for page_num in range(len(document)):\n",
    "                page = document.load_page(page_num)\n",
    "                text += page.get_text().replace('\\n', ' ').replace('\\r', ' ')  # Replace newlines and carriage returns with spaces\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            print(f'Failed to extract text from {pdf_path}: {e}')\n",
    "            return \"\"\n",
    "\n",
    "pdf_scraper = PDFScraper()\n",
    "story_data = []\n",
    "counter = 0\n",
    "\n",
    "for article_link in article_links:\n",
    "    if counter >= 10:  # Stop after scraping 10 articles\n",
    "        break\n",
    "\n",
    "    counter += 1\n",
    "    print(f'Scraping article {counter}/{len(article_links)}')\n",
    "    current_page_scraper = IRISArticleScraper(article_link)\n",
    "\n",
    "    # Download PDF if available and add to story_data\n",
    "    pdf_url = current_page_scraper.pdf\n",
    "    pdf_filename = None\n",
    "    extracted_text = None\n",
    "    if pdf_url and pdf_url != 'No pdf found':\n",
    "        pdf_filename = pdf_scraper.download_pdf(pdf_url)\n",
    "        if pdf_filename:\n",
    "            # Extract text from the downloaded PDF\n",
    "            extracted_text = pdf_scraper.extract_text_from_pdf(pdf_filename)\n",
    "            if extracted_text:\n",
    "                print(f'Extracted text from {pdf_filename}:')\n",
    "            else:\n",
    "                print(f'Failed to extract text from {pdf_filename}')\n",
    "        else:\n",
    "            print(f'Failed to download PDF from {pdf_url}')\n",
    "    else:\n",
    "        print(f'No PDF found for {current_page_scraper.title}')\n",
    "\n",
    "    # Collect data for story_data_with_pdfs\n",
    "    story_data.append({\n",
    "        'title': current_page_scraper.title,\n",
    "        'author': current_page_scraper.author,\n",
    "        'date': current_page_scraper.date,\n",
    "        'publication': current_page_scraper.publication,\n",
    "        'link': current_page_scraper.link,\n",
    "        'type': 'International',\n",
    "        'pdf': pdf_filename,  # Store the filename or None if no PDF\n",
    "        'pdf_link': current_page_scraper.link,\n",
    "        'text': extracted_text if extracted_text else 'No text extracted'\n",
    "    })\n",
    "\n",
    "print('Completed scraping articles.')\n",
    "print(story_data[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving to File (CSV)\n",
    "\n",
    "Now we need to output the data for text analysis. In this step we will output each record as a line in a .CSV (spreadsheet) file. That file will be stored in `who_iris.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Clean the text by replacing newlines and escaping quotes.\"\"\"\n",
    "    if text:\n",
    "        # Replace newlines and carriage returns with spaces\n",
    "        text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "        # Escape double quotes by replacing \" with \"\"\n",
    "        text = text.replace('\"', '\"\"')\n",
    "    return text\n",
    "\n",
    "OUTPUT_CSV_FILENAME = 'who_iris.csv'\n",
    "OUTPUT_CSV_FIELDNAMES = ['title', 'author', 'date', 'publication', 'link', 'type', 'pdf', 'pdf_link', 'text']\n",
    "\n",
    "# Specify the directory where PDFs are stored\n",
    "pdfs_directory = 'C:/Users/carverle/Documents/pdfs'\n",
    "\n",
    "# Create output directory path\n",
    "output_filepath = os.path.join(os.path.abspath(os.getcwd()), pdfs_directory, OUTPUT_CSV_FILENAME)\n",
    "\n",
    "# Ensure directory exists, if not, create it\n",
    "output_directory = os.path.dirname(output_filepath)\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "print('Writing CSV File ', output_filepath)\n",
    "try:\n",
    "    with open(output_filepath, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=OUTPUT_CSV_FIELDNAMES, quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writeheader()\n",
    "        for story_datum in story_data:\n",
    "            # Clean the text field\n",
    "            story_datum['text'] = clean_text(story_datum['text'])\n",
    "            writer.writerow(story_datum)\n",
    "    print('Success writing CSV File!')\n",
    "except IOError:\n",
    "    print(f'Error writing to CSV file: {output_filepath}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ebola",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
