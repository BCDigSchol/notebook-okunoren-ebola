{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the Awoko Newspaper\n",
    "\n",
    "**Scraping script done on behalf of Oladoyin Okunoren @ Boston College**\n",
    "\n",
    "By David J. Thomas\n",
    "\n",
    "---\n",
    "\n",
    "This notebook contains a series of scripts to scrape every news story from the Awoko Newspaper about Ebola from 2014-2016. It is a part of the dissertation of research of Oladoyin Okunoren, at [Boston College](https://bc.edu)\n",
    "\n",
    "---\n",
    "\n",
    "## Installation\n",
    "\n",
    "``` bash\n",
    "pip install -r requirements.txt\n",
    "jupyter lab\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining A General Object to Scrape Any Page\n",
    "\n",
    "Before we can get going scraping pages, this step below will create a general class called `BasePageScraper`, with a few basic functions that, when given a URL, will fetch the page's HTML, and then parse it using BeautifulSoup. This object won't be used directly. However, it will serve as a base class for objects defined in the following steps that are aimed at scraping specific pages. These child classes will pick up the methods and attributes of their parents. Once this base class is defined, we will print a success message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class BasePageScraper:\n",
    "    \"\"\"Gets HTML for a single page, parses it with BeautifulSoup, and stores results in self.data.\n",
    "    Base class for child classes which target specific pages\"\"\"\n",
    "    \n",
    "    def __init__(self, url, total_tries=5, scrape_delay=5):\n",
    "        \"\"\"Gets url and pre-parses the html\"\"\"\n",
    "        self._url = url\n",
    "        self._total_tries = total_tries\n",
    "        self._scrape_delay = scrape_delay\n",
    "        # store souped data in object for extraction\n",
    "        self.data = self.soup_page(tries_left=self._total_tries)\n",
    "\n",
    "    def soup_page(self, tries_left=5):\n",
    "        \"\"\"Receives URL, requests raw html, then returns converted BeautifulSoup object.\"\"\"\n",
    "        # declare variable for raw html\n",
    "        page_html = None\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0\",\n",
    "            \"Accept-Encoding\": \"*\",\n",
    "            \"Connection\": \"keep-alive\"\n",
    "        }\n",
    "        # ensure tries_left is set and valid, if not set to 5, check if url is valid\n",
    "        if not tries_left or type(tries_left) != int or tries_left < 0:\n",
    "            tries_left = 5\n",
    "        if type(self._url) != str:\n",
    "            raise Exception('URL must be a valid string')\n",
    "        # enforce a time delay between each scrape for good internet citizenship\n",
    "        time.sleep(self._scrape_delay)\n",
    "        print('Getting', self._url)\n",
    "        # attempt to get page data, decrement tries_left if successful\n",
    "        try:\n",
    "            page_html = requests.get(self._url, headers=headers).text\n",
    "            tries_left -= 1\n",
    "        # if an error occured, retry by returning recursively\n",
    "        except:\n",
    "            print('Error getting', self._url)\n",
    "            if tries_left > 0:\n",
    "                print('Retrying...')\n",
    "                print(tries_left)\n",
    "                return self.soup_page(self._url, tries_left=tries_left-1)\n",
    "            if tries_left <= 0:\n",
    "                print('Retry limit reached, ABORTING parse of', self._url)\n",
    "                return None\n",
    "        print('Success, souping...')\n",
    "        # if all went well, return new BeautifulSoup populated with the page html and parser set to html.parser\n",
    "        return BeautifulSoup(page_html, 'html.parser')\n",
    "\n",
    "print('Function defined! PROCEED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the Awoko Browse Page (Will take awhile)\n",
    "\n",
    "This step will find the links to each story about ebola, from 2014-2016 onwards. The scraping object defined below uses the parent class from above. Howevr, it adds a few extra functions. One gets the link from the \"next button (`.next_link`), if it exists. Another function gives a list of every link to an article on that specific browse page (`.links`). The last function, `.gather_links()`, gets all the links from the given page... and then calls itself reccursively on every following page (using the next page links) to get every link... not just those on this page, but on all following pages.\n",
    "\n",
    "Once the object is defined, it can be used to scrape all of the ebola stories for a given year. So, below the object definition, the object is used three times... on the stories from (1) 2014, (2) 2015, and (3) 2016. All of those links are gathered together in a variable called `article_links` which will be used in the next step. There, we will use another scraper object on each of those pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AwokoBrowseScraper(BasePageScraper):\n",
    "    \"\"\"Represents a browse page of the Awoko News Papers. Gathers links of stories returned from a search\"\"\"\n",
    "\n",
    "    def __init__(self, url, total_tries=5, scrape_delay=5):\n",
    "        super().__init__(url=url, total_tries=total_tries, scrape_delay=scrape_delay)\n",
    "\n",
    "    @property\n",
    "    def next_link(self):\n",
    "        \"\"\"Returns link to the next browse page if exists, or None if no further page\"\"\"\n",
    "        link = self.data.find('a', class_='next')\n",
    "        if not link:\n",
    "            return None\n",
    "        else:\n",
    "            return link['href']\n",
    "\n",
    "    @property\n",
    "    def links(self):\n",
    "        \"\"\"Peruses the souped data and returns list of strings, each with a link\"\"\"\n",
    "        return_links = []\n",
    "        for article in self.data.find_all('article', class_='jeg_post'):\n",
    "            # append link of article to list\n",
    "            return_links.append(article.find('h3', class_='jeg_post_title').find('a')['href'])\n",
    "        return return_links\n",
    "    \n",
    "    def gather_links(self):\n",
    "        \"\"\"Recursive function to gather links to all stories on this page, and subsequent pages. If not on the last page,\n",
    "        return the links on the page plus those returned by a recursively call another AwokoBrowseScraper object on the next page.\n",
    "        If on the last page, just return the links and break the recursive loop\"\"\"\n",
    "        # if no next_link, just return the links on the page\n",
    "        if not self.next_link:\n",
    "            return self.links\n",
    "        else:\n",
    "            return self.links + AwokoBrowseScraper(self.next_link).gather_links()\n",
    "\n",
    "article_links = []\n",
    "print('Gathering stories, this may take awhile...')\n",
    "# gather stories for 2014, 2015, 2016\n",
    "article_links += AwokoBrowseScraper('https://awokonewspaper.sl/page/1/?s=ebola&year=2014').gather_links()\n",
    "article_links += AwokoBrowseScraper('https://awokonewspaper.sl/page/1/?s=ebola&year=2015').gather_links()\n",
    "article_links += AwokoBrowseScraper('https://awokonewspaper.sl/page/1/?s=ebola&year=2016').gather_links()\n",
    "\n",
    "print(str(len(article_links)) + ' links to stories gathered')\n",
    "print(article_links[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Each Article (Will take a long while)\n",
    "\n",
    "Warning: this step will take a long time to run. First, we will define another scraper object, this one representing the page of an article on the Awoko Newspaper site. Once we define the object, we will the use it on each url in `article_links` gathered above. The scraper object will make it quick and easy to extract the text and also various bits of metadata.\n",
    "\n",
    "Once we have defined the object, we will loop through each of the `article_links`, calling the object, and appending all of the data it extracts as a dictionary inside of the list `story_data`. In the next steps, we will save the extracted data as both CSV and TXT files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AwokoArticleScraper(BasePageScraper):\n",
    "    \"\"\"Represents an article of the Awoko Newspaper. Aids extracting text and metadata\"\"\"\n",
    "\n",
    "    def __init__(self, url, total_tries=5, scrape_delay=5):\n",
    "        super().__init__(url=url, total_tries=total_tries, scrape_delay=scrape_delay)\n",
    "        self.data = self.data.find('div', class_='jeg_inner_content')\n",
    "\n",
    "    @property\n",
    "    def title(self):\n",
    "        return self.data.find('h1', class_='jeg_post_title').get_text()\n",
    "\n",
    "    @property\n",
    "    def author(self):\n",
    "        return self.data.find('div', class_='jeg_meta_author').a.get_text()\n",
    "\n",
    "    @property\n",
    "    def date(self):\n",
    "        return self.data.find('div', class_='jeg_meta_date').a.get_text()\n",
    "\n",
    "    @property\n",
    "    def column(self):\n",
    "        return self.data.find('div', class_='jeg_meta_category').a.get_text()\n",
    "\n",
    "    @property\n",
    "    def publication(self):\n",
    "        return 'Awoko Newspaper'\n",
    "    \n",
    "    @property\n",
    "    def link(self):\n",
    "        return self._url\n",
    "    \n",
    "    @property\n",
    "    def text(self):\n",
    "        return self.data.find('div', class_='entry-content').find('div', class_='content-inner').p.get_text()\n",
    "    \n",
    "# loop through every link, create an AwokoArtricleScraper object for it, extract data, and store in story_data\n",
    "story_data = []\n",
    "counter = 0\n",
    "for article_link in article_links:\n",
    "    counter += 1\n",
    "    print('Scraping article ' + str(counter) + '/' + str(len(article_links)))\n",
    "    current_page_scraper = AwokoArticleScraper(article_link)\n",
    "    story_data.append({\n",
    "        'title': current_page_scraper.title,\n",
    "        'author': current_page_scraper.author,\n",
    "        'date': current_page_scraper.date,\n",
    "        'column': current_page_scraper.column,\n",
    "        'publication': current_page_scraper.publication,\n",
    "        'link': current_page_scraper.link,\n",
    "        'type': 'local',\n",
    "        'text': current_page_scraper.text\n",
    "    })\n",
    "\n",
    "print(story_data[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving to File (CSV)\n",
    "\n",
    "Now we need to output the data for text analysis. In this step we will output each record as a line in a .CSV (spreadsheet) file. That file will be stored in `output/awoko_newspaper.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "OUTPUT_CSV_FILENAME = 'awoko_newspaper.csv'\n",
    "OUTPUT_CSV_FIELDNAMES = ['title', 'date', 'author', 'column', 'publication', 'link', 'type', 'text']\n",
    "\n",
    "output_filepath = os.path.join(os.path.abspath(os.getcwd()), 'output', OUTPUT_CSV_FILENAME)\n",
    "\n",
    "# ensure directory exists, if not, create it\n",
    "if not os.path.exists(os.path.join(os.path.abspath(os.getcwd()), 'output')):\n",
    "    os.makedirs(os.path.join(os.path.abspath(os.getcwd()), 'output'))\n",
    "\n",
    "print('Writing CSV File ', output_filepath)\n",
    "with open(output_filepath, 'w+', encoding='utf8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=OUTPUT_CSV_FIELDNAMES)\n",
    "    writer.writeheader()\n",
    "    for story_datum in story_data:\n",
    "        writer.writerow(story_datum)\n",
    "\n",
    "print('Success writing CSV File!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving to File (TXT)\n",
    "\n",
    "Finally, some text analysis packages use folders of .txt files, instead of .csv files. So, we will also output every record as a .txt file that will be located inside of `output/awoko_newspaper/FILENAME.txt`, where the FILENAME will be determined by the url to the story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OUTPUT_FOLDERNAME = 'awoko_newspaper'\n",
    "\n",
    "output_folderpath = os.path.join(os.path.abspath(os.getcwd()), 'output', OUTPUT_FOLDERNAME)\n",
    "\n",
    "# ensure directory exists, if not, create it\n",
    "if not os.path.exists(output_folderpath):\n",
    "    os.makedirs(output_folderpath)\n",
    "\n",
    "print('Writing TXT Files ', output_folderpath)\n",
    "for story_datum in story_data:\n",
    "    output_filename = story_datum['link'].split('/')[-2] + '.txt'\n",
    "    output_filepath = os.path.join(output_folderpath, output_filename)\n",
    "    txtfile = open(output_filepath, 'w+', encoding='utf8')\n",
    "    txtfile.write(story_datum['text'])\n",
    "    txtfile.close()\n",
    "\n",
    "print('Success writing TXT Files!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ebola",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
